apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
    name: rook-ceph-cluster
    namespace: rook-ceph
spec:
    interval: 30m
    timeout: 15m
    releaseName: rook-ceph-cluster
    chart:
        spec:
            # renovate: registryUrl=https://charts.rook.io/release
            chart: rook-ceph-cluster
            version: v1.18.9
            sourceRef:
                kind: HelmRepository
                name: rook-ceph
                namespace: flux-system
    install:
        remediation:
            retries: 3
    upgrade:
        remediation:
            retries: 3
    values:
      toolbox:
        enabled: true
      removeOSDsIfOutAndSafeToRemove: true
      monitoring:
        enabled: true
        createPrometheusRules: false
      configOverride: |
        [global]
        bdev_enable_discard = true
        bdev_async_discard = true
        osd_class_update_on_start = false
        bluestore_compression_algorithm = lz4
        bluestore_compression_mode = aggressive
        bluestore_compression_required_ratio = .875
        rbd_default_stripe_unit = 65536
        rbd_default_stripe_count = 16
        # Memory optimization settings
        [osd]
        # Limit OSD memory usage to 512Mi base target
        osd_memory_target = 536870912
        # Enable automatic memory tuning to adapt to workload
        osd_memory_target_autotune = true
        # Control number of PGs to reduce memory overhead
        mon_max_pg_per_osd = 200
      cephClusterSpec:
        mgr:
          count: 1
          allowMultiplePerNode: false
        mon:
          count: 1
          allowMultiplePerNode: false
        resources:
          mgr:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              memory: 2Gi
          # Increase operator memory to prevent OOMKilled crashes
          operator:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          mon:
            requests:
              cpu: 50m
              memory: 512Mi
            limits:
              memory: 1Gi
          osd:
            requests:
              cpu: 500m
              memory: 512Mi
            limits:
              memory: 2Gi
          prepareosd:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 51Mi
          mgr-sidecar:
            requests:
              cpu: 50m
              memory: 128Mi
            limits:
              memory: 256Mi
          crashcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "64M"
          logcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"
          cleanup:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"

        crashCollector:
          disable: false

        dashboard:
          enabled: true
          urlPrefix: /
          ssl: false

        storage:
          useAllNodes: true
          useAllDevices: true
          allowDeviceClassUpdate: true
          allowOsdCrushWeightUpdate: true
          enableCrushUpdates: true
          # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
          onlyApplyOSDPlacement: false
          config:
            deviceClass: nvme

      # ingress:
      #   dashboard:
      #     ingressClassName: internal #added this to prevent external default when using nginx in annotations
      #     annotations:
      #       # nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      #       kubernetes.io/ingress.class: internal #was nginx
      #       cert-manager.io/cluster-issuer: domain-0-le-prod
      #       cert-manager.io/private-key-rotation-policy: Always
      #       nginx.ingress.kubernetes.io/ssl-redirect: "true"
      #       # traefik.ingress.kubernetes.io/router.middlewares: traefik-chain-basic@kubernetescrd
      #     tls:
      #       - hosts:
      #           - rook-ceph.${DOMAIN_1}
      #         secretName: rook-ceph-dashboard
      #     host:
      #       name: "rook-ceph.${DOMAIN_1}"
      #       path: "/"
      cephBlockPools:
        - name: ceph-nvme
          spec:
            failureDomain: host
            deviceClass: nvme
            enableRBDStats: true
            parameters:
              compression_mode: aggressive
            replicated:
              size: 3
              requireSafeReplicaSize: false
          storageClass:
            enabled: true
            name: ceph-nvme
            isDefault: true
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            volumeBindingMode: Immediate
            parameters:
              imageFormat: "2"
              imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/fstype: ext4
      cephBlockPoolsVolumeSnapshotClass:
        enabled: true
        name: csi-ceph-blockpool
        isDefault: true
        deletionPolicy: Delete

      cephFileSystemVolumeSnapshotClass:
        enabled: true
        name: ceph-filesystem
        isDefault: true
        deletionPolicy: Delete

      cephFileSystems:
        - name: ceph-filesystem
          # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
          spec:
            metadataPool:
              replicated:
                size: 3
            dataPools:
              - failureDomain: host
                replicated:
                  size: 3
                # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
                name: data0
            metadataServer:
              activeCount: 1
              activeStandby: true
              resources:
                limits:
                  memory: "1Gi"
                requests:
                  cpu: "1000m"
                  memory: "512Mi"
              priorityClassName: system-cluster-critical
          storageClass:
            enabled: true
            isDefault: false
            name: ceph-filesystem
            # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
            pool: data0
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            volumeBindingMode: "Immediate"
            annotations: {}
            labels: {}
            mountOptions: []
            # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
            parameters:
              # The secrets contain Ceph admin credentials.
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
              csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
              # Specify the filesystem type of the volume. If not specified, csi-provisioner
              # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
              # in hyperconverged settings where the volume is mounted on the same node as the osds.
              csi.storage.k8s.io/fstype: ext4
